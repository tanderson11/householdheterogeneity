{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17403acf",
   "metadata": {},
   "source": [
    "# Fitting to empirical data\n",
    "\n",
    "This notebook is used to generate maximum likelihood estimates for the parameters of a dynamic model judging from empirically collected data. It's used to produce Table 1, Figure 4, and supplemental contour figures in the accompanying paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9322b3c",
   "metadata": {
    "id": "DVNH5-KSBUlt"
   },
   "source": [
    "## Google Colab initialization\n",
    "\n",
    "This section will help you interface with Google Drive and clone the git repository where the code lives. These steps **aren't necessary if you are running locally**. First, make sure you have opened the notebook in Google Colab (use the below button if ncessary) and logged into your Google account.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/tanderson11/covidhouseholds/blob/main/notebooks/EmpiricalFits.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4eee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORzYVf06BhuU",
    "outputId": "47fd409f-f438-4582-bd24-a5414f5cd496"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810bfd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3fmCBmpDCZn",
    "outputId": "ab0b2183-1467-44f7-854b-01932385229e"
   },
   "outputs": [],
   "source": [
    "%mkdir /content/gdrive/My\\ Drive/github/\n",
    "%cd /content/gdrive/My\\ Drive/github/\n",
    "# Thayer has his files located here instead\n",
    "#%cd /content/gdrive/My\\ Drive/github/paper_push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f3005",
   "metadata": {
    "id": "J0qVcYVio4mo"
   },
   "outputs": [],
   "source": [
    "# If you've forked the repository, point to your own username and repository name (if different)\n",
    "repo_owner=\"tanderson11\"\n",
    "repository=\"covidhouseholds\"\n",
    "\n",
    "!git config --global user.email \"tanderson11@gmail.com\"\n",
    "!git config --global user.name \"Thayer Anderson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b982537f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wBI8yq891xZ",
    "outputId": "ff975c41-eafa-4049-a218-1ec35c06a94d"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/tanderson11/covidhouseholds.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858d803",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqQubdG6EpNl",
    "outputId": "897a3f56-7399-4d30-c3a4-45adc1e711fb"
   },
   "outputs": [],
   "source": [
    "%cd covidhouseholds/\n",
    "!ls -a\n",
    "\n",
    "# >>> TOKEN SETUP: <<<\n",
    "# this will put your token in the right folder; comment this line out after use to avoid an error message\n",
    "#!mv ../git_token.py ./\n",
    "#!cp ../../covidhouseholds/git_token.py ./\n",
    "\n",
    "#from git_token import git_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b26d12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYatPc_m-Iwo",
    "outputId": "0666afa8-1fa4-4a72-99a9-125902f5f4a3"
   },
   "outputs": [],
   "source": [
    "!git checkout main\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ae4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8ebdc",
   "metadata": {},
   "source": [
    "## Module initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7478c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thayer/develop/covid_households\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'likelihood'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpq\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mcd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m../\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlikelihood\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlikelihood\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcovidhouseholds\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecipes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mrecipes\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'likelihood'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "%cd ../\n",
    "import src.likelihood as likelihood\n",
    "import src.recipes as recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d14dc5",
   "metadata": {},
   "source": [
    "## Loading empirical and simulated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.abspath('')\n",
    "results = recipes.Results.load(os.path.join(root, 'new_parameters/gillespie-s80-p80-SAR/beta_corrections'))\n",
    "results.find_frequencies(inplace=True)\n",
    "\n",
    "keys = results.metadata.parameters\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data as a pandas dataframe\n",
    "population_name = 'Ontario'\n",
    "if population_name == 'Ontario':\n",
    "    empirical_df = os.path.join(root, \"empirical/Ontario/empirical_df.parquet\")\n",
    "    empirical_df = pq.read_table(empirical_df).to_pandas()\n",
    "elif population_name == 'Bnei Brak':\n",
    "    results = recipes.Results.load(os.path.join(root, 'new_parameters/gillespie-s80-p80-SAR/beta_corrections/high_sizes'))\n",
    "    results.find_frequencies(inplace=True)\n",
    "\n",
    "    dfs = []\n",
    "    n_random_dfs = 20\n",
    "    for i in range(n_random_dfs):\n",
    "        randomized_df_path = os.path.join(root, f\"empirical/BneiBrak/randomized_corrections/randomized_df{i}.parquet\")\n",
    "        randomized_df = pq.read_table(randomized_df_path).to_pandas()\n",
    "        dfs.append(randomized_df)\n",
    "    empirical_df = pd.concat(dfs)\n",
    "    print(empirical_df)\n",
    "    true_path = os.path.join(root, f\"empirical/BneiBrak/empirical_df.parquet\")\n",
    "    bnei_brak_true_hhs = pq.read_table(true_path).to_pandas()\n",
    "elif population_name == 'Geneva':\n",
    "    empirical_df = os.path.join(root, \"empirical/geneva/empirical_df.parquet\")\n",
    "    empirical_df = pq.read_table(empirical_df).to_pandas()\n",
    "else:\n",
    "    assert ValueError(\"population name is not recognized.\")\n",
    "\n",
    "# the infections column should be integer values\n",
    "empirical_df['infections'] = empirical_df['infections'].round(0)\n",
    "empirical_df = empirical_df.astype({'infections': 'int32', 'size': 'int32'})\n",
    "\n",
    "# for the different parameter values, we don't know their true value, so let's give them the dummy value of 0.\n",
    "for key in keys:\n",
    "    empirical_df[key] = 0.\n",
    "\n",
    "# turn the list of households into counts of the number of infections in different households\n",
    "empirical_counts = likelihood.counts_from_empirical(empirical_df, keys, sample_only_keys=[])\n",
    "\n",
    "if population_name == 'Bnei Brak':\n",
    "    empirical_counts = empirical_counts[empirical_counts.index.get_level_values('size') <= 15]\n",
    "    empirical_counts = empirical_counts / 20\n",
    "empirical_counts.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dcaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_secondary_contacts(empirical_counts):\n",
    "    counts = empirical_counts.reset_index()\n",
    "    return ((counts['size'] - 1) * counts['count']).sum()\n",
    "\n",
    "def secondary_contacts_infected(empirical_counts):\n",
    "    counts = empirical_counts.reset_index()\n",
    "    return ((counts['infections'] - 1) * counts['count']).sum()\n",
    "\n",
    "n = total_secondary_contacts(empirical_counts)\n",
    "X = secondary_contacts_infected(empirical_counts)\n",
    "\n",
    "n,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ccf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep(n, X):\n",
    "    p_hat = X/n\n",
    "    q_hat = 1 - p_hat\n",
    "    sep = np.sqrt(p_hat * q_hat/n)\n",
    "\n",
    "    return sep\n",
    "\n",
    "# Careful not to use the randomized Bnei Brak data. Here are the actual results:\n",
    "# Bnei Brak: secondary contacts infected = 975\n",
    "# Bnei Brak: total # secondary contacts = 2716\n",
    "\n",
    "sep(n,X) * 2 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b38aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5112822",
   "metadata": {},
   "source": [
    "Sometimes for fun, we want to look at the best fit under a \"null hypothesis\" of no heterogeneity. In order to do that, we need to select the results of our simulations that have no heterogeneity ($s_{80} = p_{80} = 80\\%$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc480461",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = results.df['frequency']\n",
    "s80_l = freq.index.get_level_values(0)\n",
    "p80_l = freq.index.get_level_values(1)\n",
    "\n",
    "null_freqs = freq[(s80_l == 0.8) & (p80_l == 0.8)]\n",
    "null_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5d053",
   "metadata": {},
   "source": [
    "## Constructing likelihood surface and finding MLEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742b5d9",
   "metadata": {},
   "source": [
    "By default, the log likelihood is held in a `Series` of log likelihood values organized around a 4-level index. The first index represent which empirical trial corresponds to that section of data. The next three parameters correspond to the parameter values at which we are estimating the likelihood (ie the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0cc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the log likelihood using the table of frequencies calculated over many households and the empirical counts from the dataset\n",
    "logl = likelihood.logl_from_frequencies_and_counts(results.df['frequency'], empirical_counts, keys)\n",
    "# by default indexed by a 'trial' index, but there is only one trial (the 0th) because this is a single dataset\n",
    "logl = logl.loc[0]\n",
    "logl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ce4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"profile\" likelihood: looking at fixing two parameters at the MLE and trying to get 95% confidence in the last one\n",
    "in_95 = likelihood.normalize_probability(logl).sort_values(ascending=False).head()\n",
    "in_95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8a075",
   "metadata": {},
   "source": [
    "The parameters of the maximum likelihood estimate are easy to obtain by just finding the maximum value in the table of likelihood values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bedec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle = logl.idxmax()\n",
    "keys, mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aff69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_logl = likelihood.logl_from_frequencies_and_counts(null_freqs, empirical_counts, keys).loc[0]\n",
    "null_mle = null_logl.idxmax()\n",
    "null_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_95_mask = likelihood.confidence_mask_from_logl(logl, percentiles=(0.95,)).astype('bool')\n",
    "confidence_95_intervals = [\n",
    "    likelihood.confidence_interval_from_confidence_mask(confidence_95_mask, 's80'),\n",
    "    likelihood.confidence_interval_from_confidence_mask(confidence_95_mask, 'p80'),\n",
    "    likelihood.confidence_interval_from_confidence_mask(confidence_95_mask, 'SAR'),\n",
    "]\n",
    "keys, confidence_95_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f93330",
   "metadata": {},
   "source": [
    "## Bar graphs of empirically observed infections and best fit simulated infections\n",
    "\n",
    "To construct these graphs, we have to organize the empirical data and the simulated data into a single DataFrame and then plot the observed/expected infections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the frequencies at the mle give the proportions of different # of infections in the best fit from the model\n",
    "best_fit_infections = results.df.loc[mle]['frequency']\n",
    "best_fit_infections.name = \"model\"\n",
    "best_fit_infections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14beb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the frequencies at the null hypothesis mle give the proportions of different # of infections in the best fit from the model\n",
    "null_fit_infections = results.df.loc[null_mle]['frequency']\n",
    "null_fit_infections.name = \"null model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the empirical data is observed counts, we convert that to frequencies by dividing the counts by the total number of households of that size\n",
    "empirical_frequencies = empirical_counts/(empirical_counts.groupby(['sample ' + k for k in keys]+[\"size\"]).sum())\n",
    "empirical_frequencies.name = 'data'\n",
    "# we have to index past the dummy key values that we assigned earlier for each parameter in the empirical dataset\n",
    "empirical_frequencies = empirical_frequencies.loc[0.0, 0.0, 0.0]\n",
    "empirical_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "include_null = False\n",
    "if include_null:\n",
    "    combined_best_and_empirical = pd.concat([empirical_frequencies, best_fit_infections, null_fit_infections], axis=1)\n",
    "else:\n",
    "    combined_best_and_empirical = pd.concat([empirical_frequencies, best_fit_infections], axis=1)\n",
    "empirical_sizes = empirical_df.groupby('size').size()\n",
    "n_sizes = len(empirical_sizes.index.unique())\n",
    "\n",
    "which_sizes = empirical_sizes.index.unique()\n",
    "which_sizes = [2, 5, 8]\n",
    "\n",
    "vertical = True\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(which_sizes) / n_cols))\n",
    "\n",
    "if vertical:\n",
    "    n_cols, n_rows = n_rows, n_cols\n",
    "\n",
    "\n",
    "fixed_ylims = False\n",
    "ylims_by_household_size = {\n",
    "    2: 6000,\n",
    "    5: 3000,\n",
    "    8: 300\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols)\n",
    "axis_generator = (ax for ax in np.ravel(axes))\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "color_dict = {'model': cmap(0), 'data': cmap(1), 'null model':cmap(2)}\n",
    "for k,g in combined_best_and_empirical.groupby('size'):\n",
    "    if k not in which_sizes:\n",
    "        continue\n",
    "    try:\n",
    "        ax = next(axis_generator)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    try:\n",
    "        title = f\"Household size = {k} (# hh in data = {empirical_sizes[k]})\"\n",
    "    except KeyError:\n",
    "        continue\n",
    "    g = g.set_index(g.index.droplevel('size')) * empirical_sizes[k]\n",
    "    g = g / g.sum()\n",
    "    g.plot.bar(\n",
    "        ax=ax,\n",
    "        figsize=(5*n_cols,n_rows*4),\n",
    "        title=title,\n",
    "        xlabel=\"infections\",\n",
    "        ylabel=\"fraction\",\n",
    "        sharex=True,\n",
    "        color=color_dict,\n",
    "    ),\n",
    "    if fixed_ylims:\n",
    "        ax.set_ylim((0,  ylims_by_household_size[k]))\n",
    "    #plt.ylim((0,6000))\n",
    "\n",
    "plt.savefig(f'./figures/{population_name}_data.pdf', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecounts = empirical_counts.loc[0.0, 0.0, 0.0]\n",
    "(ecounts.groupby('size').sum()/(ecounts.groupby('size').sum().sum())).plot.bar(color='green')\n",
    "plt.ylim(0, 0.5)\n",
    "plt.savefig(f'./figures/{population_name}_household_histogram.pdf', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70e435",
   "metadata": {},
   "source": [
    "## Contours of the likelihood surface for a given parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa72352",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = likelihood.normalize_probability(logl)\n",
    "prob_df = prob_df.groupby(['s80', 'SAR']).sum()\n",
    "prob_df = prob_df / prob_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = prob_df.unstack()\n",
    "\n",
    "X,Y = np.meshgrid(Z.columns, Z.index)\n",
    "#print(self.kwargs)\n",
    "contourf = plt.contourf(X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1b0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e0418455f80b5f7c07ffd089975cbf09b69b8ff5f9769ad65d5982a17cefd6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
