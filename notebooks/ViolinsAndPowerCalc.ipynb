{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab initialization\n",
    "\n",
    "This section will help you interface with Google Drive and clone the git repository where the code lives. These steps **aren't necessary if you are running locally**. First, make sure you have opened the notebook in Google Colab (use the below button if ncessary) and logged into your Google account.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/tanderson11/covidhouseholds/blob/main/notebooks/ViolinsAndPowerCalc.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORzYVf06BhuU",
    "outputId": "47fd409f-f438-4582-bd24-a5414f5cd496"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3fmCBmpDCZn",
    "outputId": "ab0b2183-1467-44f7-854b-01932385229e"
   },
   "outputs": [],
   "source": [
    "%mkdir /content/gdrive/My\\ Drive/github/\n",
    "%cd /content/gdrive/My\\ Drive/github/\n",
    "# Thayer has his files located here instead\n",
    "#%cd /content/gdrive/My\\ Drive/github/paper_push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0qVcYVio4mo"
   },
   "outputs": [],
   "source": [
    "# If you've forked the repository, point to your own username and repository name (if different)\n",
    "repo_owner=\"tanderson11\"\n",
    "repository=\"householdheterogeneity\"\n",
    "\n",
    "!git config --global user.email \"tanderson11@gmail.com\"\n",
    "!git config --global user.name \"Thayer Anderson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wBI8yq891xZ",
    "outputId": "ff975c41-eafa-4049-a218-1ec35c06a94d"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/{repo_owner}/{repository}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqQubdG6EpNl",
    "outputId": "897a3f56-7399-4d30-c3a4-45adc1e711fb"
   },
   "outputs": [],
   "source": [
    "%cd householdheterogeneity/\n",
    "!ls -a\n",
    "\n",
    "# >>> TOKEN SETUP: <<<\n",
    "# this will put your token in the right folder; comment this line out after use to avoid an error message\n",
    "#!mv ../git_token.py ./\n",
    "#!cp ../../householdheterogeneity/git_token.py ./\n",
    "\n",
    "#from git_token import git_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYatPc_m-Iwo",
    "outputId": "0666afa8-1fa4-4a72-99a9-125902f5f4a3"
   },
   "outputs": [],
   "source": [
    "!git checkout main\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iO7I7oftLV1d"
   },
   "source": [
    "# Package initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../\n",
    "import src.recipes as recipes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['pdf.fonttype']=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model_inputs as model_inputs\n",
    "#import rebuild\n",
    "s80_axis = np.linspace(0.02, 0.80, 40)\n",
    "p80_axis = np.linspace(0.02, 0.80, 40)\n",
    "sar_axis = np.linspace(0.01, 0.60, 60)\n",
    "axes_by_key = {'s80':s80_axis, 'p80':p80_axis, 'SAR':sar_axis}\n",
    "big_region = recipes.SimulationRegion(axes_by_key, model_inputs.S80_P80_SAR_Inputs)\n",
    "\n",
    "#try:\n",
    "#    high_size_results = rebuild.rebuild(rebuild.gillespie_high_size_completed_dirs, rebuild.gillespie_high_size_dirs, './new_parameters/gillespie-s80-p80-SAR/beta_corrections/high_sizes')\n",
    "#except rebuild.MissingDataException as e:\n",
    "#    exception = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_rebuild = False\n",
    "import src.model_inputs as model_inputs\n",
    "#import rebuild\n",
    "if do_rebuild:\n",
    "    s80_axis = np.linspace(0.02, 0.80, 40)\n",
    "    p80_axis = np.linspace(0.02, 0.80, 40)\n",
    "    sar_axis = np.linspace(0.01, 0.60, 60)\n",
    "    axes_by_key = {'s80':s80_axis, 'p80':p80_axis, 'SAR':sar_axis}\n",
    "    big_region = recipes.SimulationRegion(axes_by_key, model_inputs.S80_P80_SAR_Inputs)\n",
    "\n",
    "    small_s80_axis = np.linspace(0.20, 0.80, 31)\n",
    "    small_p80_axis = np.linspace(0.20, 0.80, 31)\n",
    "    small_sar_axis = np.linspace(0.10, 0.60, 51)\n",
    "    small_axes_by_key = {'s80':small_s80_axis, 'p80':small_p80_axis, 'SAR':small_sar_axis}\n",
    "    small_region = recipes.SimulationRegion(small_axes_by_key, model_inputs.S80_P80_SAR_Inputs)\n",
    "\n",
    "    results = rebuild.rebuild(rebuild.gillespie_completed_dirs, rebuild.gillespie_from_parts_dirs, './new_parameters/gillespie-s80-p80-SAR/beta_corrections', overwrite_dirs=rebuild.gillespie_overwrite_dirs, check_region=big_region)\n",
    "    #results = rebuild.rebuild(rebuild.tweaked_dprob_completed_dirs, rebuild.tweaked_dprob_from_parts_dirs, './new_parameters/s80-p80-SAR-sizes-2-8-tweaked-dprobability', check_region=small_region)\n",
    "else:\n",
    "    #results = recipes.Results.load('./new_parameters/s80-p80-SAR-sizes-2-8-tweaked-dprobability')\n",
    "    #results = recipes.Results.load('./new_parameters/gillespie-s80-p80-SAR')\n",
    "    results = recipes.Results.load('./new_parameters/gillespie-s80-p80-SAR/beta_corrections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.df.loc[0.02, 0.04, 0.11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.df.loc[0.08, 0.5, 0.01, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.find_frequencies(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin figures\n",
    "\n",
    "The cells in this section are used to generate violin plots (as in Figure 3 in the paper). The first block of cells configures the simluated data set, the second block of cells produces the simulated data, and the third block of cells plots the actual figures.\n",
    "\n",
    "To change the parameters of the violin plots, change the values of `population_mixes` (what population structure to use for the simulated data), `sample_sizes` (the # of individuals to simulate in the population divided according to the population mix) and `parameter_sets` (the tuples of ($s_{80}$, $p_{80}$, $\\text{SAR}$) to simulate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepartion for running violin fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`null_freqs` represents the frequencies we observe in the absence of heterogeneity. We use it to calculate the MLE along the restriction where heterogeneity is not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = results.df['frequency']\n",
    "s80_l = freq.index.get_level_values(0)\n",
    "p80_l = freq.index.get_level_values(1)\n",
    "\n",
    "null_freqs = freq[(s80_l == 0.8) & (p80_l == 0.8)]\n",
    "null_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mles(logl, population, parameter_set, population_name=None):\n",
    "    \"\"\"Takes the log likelihood surface for each configuration and returns the MLEs (one for each trial).\n",
    "\n",
    "    Args:\n",
    "        logl (Pandas.DataFrame): the loglikelihood surface. Indexed by at least `trial` which represent different observations\n",
    "        population (dict): a dictionary of household size --> number of households.\n",
    "        parameter_set (tuple): the value of each parameter that in fact produced the simulated data\n",
    "\n",
    "    Returns:\n",
    "        Pandas.DataFrame: a dataframe of MLE values with one column for each parameter inferred\n",
    "    \"\"\"\n",
    "    fits = logl.groupby('trial').idxmax()\n",
    "    fits = pd.DataFrame(fits.tolist())\n",
    "    new_names = []\n",
    "    for name in logl.index.names:\n",
    "        name = name if name == 'trial' else 'MLE_' + name\n",
    "        new_names.append(name)\n",
    "    fits.columns = new_names\n",
    "    fits.set_index('trial')\n",
    "\n",
    "    fits['sample size'] = sum([k*v for k,v in population.items()])\n",
    "    if population_name is not None:\n",
    "        fits['population mix'] = population_name\n",
    "    else:\n",
    "        fits['population mix'] = [tuple(population.keys()) for i in range(len(fits))]\n",
    "    fits ['parameters'] = [parameter_set] * len(fits)\n",
    "    return fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the different combinations of parameters / sample sizes / household sizes to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Two  | Three\t Four     Five    Six    Seven or more\n",
    "# 2021\n",
    "# 45,515 | 19,523 | 16,098 | 7,577 | 2,635 |     1,611\n",
    "# https://www.census.gov/data/tables/time-series/demo/families/households.html\n",
    "# Let's take households of size > 2 and assume that 'seven or more' --> equal mix of 7 and 8\n",
    "American_households_size_ge_3 = {3:19523, 4:16098, 5:7577, 6:2635, 7:1611//2, 8:1611//2}\n",
    "America_households_total = np.sum([v for v in American_households_size_ge_3.values()])\n",
    "named_populations = {'America_census':{k:v/America_households_total for k,v in American_households_size_ge_3.items()}}\n",
    "population_descriptions = {'America_census': 'divided in proportion to American households of size >=3'}\n",
    "# Here we do the same thing for America but we take households of size >= 2\n",
    "American_households_size_ge_2 = {2:45515, 3:19523, 4:16098, 5:7577, 6:2635, 7:1611//2, 8:1611//2}\n",
    "America_households_total_incl_2 = np.sum([v for v in American_households_size_ge_2.values()])\n",
    "named_populations['America_census_incl_2'] = {k:v/America_households_total_incl_2 for k,v in American_households_size_ge_2.items()}\n",
    "population_descriptions['America_census_incl_2'] = 'divided in proportion to American households of size >=2'\n",
    "\n",
    "# UN data: https://www.un.org/development/desa/pd/data/household-size-and-composition\n",
    "\n",
    "# A few other countries\n",
    "# Percent 1 | 2-3 | 4-5 | 6+\n",
    "# Philippines [2017] (average size = 4.23)\n",
    "# 9.18\t30.05\t36.82\t23.95\n",
    "philippines_percents = [9.18, 30.05, 36.82, 23.95]\n",
    "# Guatemala [2015] (average size = 4.8)\n",
    "# 4.33\t26.57\t37.51\t31.60\n",
    "guatemala_percents = [4.33, 26.57, 37.51, 31.60]\n",
    "guatemala_average = 4.8\n",
    "# US [2015] (average size = 2.49)\n",
    "# 27.89\t49.49\t18.81\t3.81\n",
    "us_percents = [27.89, 49.49, 18.81, 3.81]\n",
    "# Mexico [2015] (Average size 3.74)\n",
    "# 10.08\t37.68\t37.75\t14.49\n",
    "mexico_percents = [10.08, 37.68, 37.75, 14.49]\n",
    "\n",
    "# Let's be dumb about it and just assume that you are evenly mixed within the bucket and that 6+ stops at 6\n",
    "# Don't anchor to the average, just calculate the residual\n",
    "\n",
    "buckets = [(1,1), (2,3), (4,5), (6,6)]\n",
    "def uniform_buckets(percents, left_cutoff=None):\n",
    "    np_population = np.zeros(10)\n",
    "    for i,bucket in enumerate(buckets):\n",
    "        left, right = bucket\n",
    "        average_percent = percents[i] / (right - left + 1)\n",
    "        for size in range(left, right+1):\n",
    "            np_population[size] = average_percent\n",
    "    if left_cutoff is not None:\n",
    "        np_population[:left_cutoff] = 0\n",
    "        remaining_percent = np.sum(np_population)\n",
    "        np_population = np_population * 100 / (remaining_percent)\n",
    "    population = {}\n",
    "    for size, percent in enumerate(np_population):\n",
    "        if percent == 0:\n",
    "            continue\n",
    "        population[size] = percent/100\n",
    "    return population\n",
    "\n",
    "named_populations.update({\n",
    "    'America_UN':uniform_buckets(us_percents, left_cutoff=3),\n",
    "    'Mexico_incl_2':uniform_buckets(mexico_percents, left_cutoff=2),\n",
    "    'Philippines_incl_2':uniform_buckets(philippines_percents, left_cutoff=2),\n",
    "    'Guatemala_incl_2':uniform_buckets(guatemala_percents, left_cutoff=2),\n",
    "})\n",
    "\n",
    "population_descriptions.update({\n",
    "    'America_UN':'US households size >= 3 estimated from UN',\n",
    "    'Mexico_incl_2':'Mexican households size >= 2 estimated from UN',\n",
    "    'Philippines_incl_2':'Philippine households size >= 2 estimated from UN',\n",
    "    'Guatemala_incl_2':'Guatemalan households size >= 2 estimated from UN'\n",
    "})\n",
    "\n",
    "def fraction_of_households_to_fraction_of_people(fraction_of_households_by_size):\n",
    "    times_size = {k:v*k for k,v in fraction_of_households_by_size.items()}\n",
    "    total = sum(list(times_size.values()))\n",
    "    fraction_of_people_by_size = {k:v/total for k,v in times_size.items()}\n",
    "    return fraction_of_people_by_size\n",
    "\n",
    "def population_mix_to_population(mix, sample_size):\n",
    "    fraction_of_people_by_size = fraction_of_households_to_fraction_of_people(mix)\n",
    "    print(fraction_of_people_by_size)\n",
    "    population = {k:int(np.round((v * sample_size)/k)) for k,v in fraction_of_people_by_size.items()}\n",
    "    population = {k:v for k,v in population.items() if v != 0}\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([k*v/America_households_total_incl_2 for k,v in American_households_size_ge_2.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit this cell to reconfigure the violin plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_sizes = [1000, 5000, 10000]\n",
    "sample_sizes = [5000]\n",
    "parameter_sets = [(0.2, 0.2, 0.20), (0.8, 0.2, 0.20), (0.2, 0.8, 0.20)]\n",
    "population_mixes = ['America_census_incl_2']\n",
    "\n",
    "#population_mixes = [(4,5,6,7,8), 'America_UN', 'Mexico', 'Philippines', 'Guatemala']\n",
    "#population_mixes = [(8,), (4,), 'Guatemala']#, 'America_census', 'Guatemala']\n",
    "population_mixes = [(2,3,4,), 'America_census_incl_2', (3,4,5,), 'Guatemala_incl_2', (4,5,6,)]\n",
    "#population_mixes = [(2,3,4,), (3,4,5,), (4,5,6,)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the fits\n",
    "\n",
    "Run the fits at all the combinations (create the population sample using statistical resampling — not forward simulation). Aggregate fits against the full surface in `fits_dfs` and the fits on the restriction that there is no heterogeneity in `null_hypoth_fits_dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.likelihood as likelihood\n",
    "\n",
    "trials = 1400\n",
    "#trials = 200\n",
    "\n",
    "# a hypothesis is a theory about which parameters are involved. fits will be done separately for each hypothesis\n",
    "hypotheses = {\n",
    "    'all': ['s80', 'p80', 'SAR'],\n",
    "    #'SAR and infectivity vary': ['p80', 'SAR'],\n",
    "    #'SAR and susceptibility vary': ['s80', 'SAR'],\n",
    "    'null hypothesis': ['SAR'],\n",
    "}\n",
    "\n",
    "def restrict_parameters(base_results, included_parameters):\n",
    "    freqs = base_results.df['frequency'].copy()\n",
    "\n",
    "    for parameter in set(base_results.metadata.parameters) - set(included_parameters):\n",
    "        if parameter not in ['s80', 'p80']:\n",
    "            raise ValueError(\"can't exclude SAR as it has no default hypothesis.\")\n",
    "        parameter_level = freqs.index.get_level_values(base_results.metadata.parameters.index(parameter))\n",
    "        freqs = freqs[(parameter_level == 0.8)]\n",
    "\n",
    "    return freqs\n",
    "\n",
    "\n",
    "frequencies_by_hypothesis = {k: restrict_parameters(results, included_parameters) for k,included_parameters in hypotheses.items()}\n",
    "from collections import defaultdict\n",
    "fit_collections = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_size in sample_sizes:\n",
    "    for population_mix in population_mixes:\n",
    "        population_name = None\n",
    "        if isinstance(population_mix, str):\n",
    "            population_name = population_mix\n",
    "            population = named_populations[population_mix]\n",
    "            population = population_mix_to_population(population, sample_size)\n",
    "            print(population)\n",
    "        else:\n",
    "            population_per_size = sample_size // len(population_mix)\n",
    "            population = {s:population_per_size//s for s in population_mix}\n",
    "            print(population)\n",
    "        for parameter_set in parameter_sets:\n",
    "            print(parameter_set, population)\n",
    "            samples = results.resample(parameter_set, population, trials=trials)\n",
    "            for hypothesis_name, hypothesis_frequencies in frequencies_by_hypothesis.items():\n",
    "                logl = likelihood.logl_from_frequencies_and_counts(hypothesis_frequencies, samples['count'], results.metadata.parameters)\n",
    "                fits = make_mles(logl, population, parameter_set, population_name=population_name)\n",
    "\n",
    "                normalized_probability = logl.groupby('trial').apply(lambda g: likelihood.normalize_probability(g))\n",
    "                confidence_masks = normalized_probability.groupby('trial').apply(lambda g: likelihood.find_confidence_mask(g)).astype('bool')\n",
    "                confidence_grouped = confidence_masks.groupby('trial')\n",
    "                p80_confidence_intervals = confidence_grouped.apply(lambda g: likelihood.confidence_interval_from_confidence_mask(g, 'p80', include_endpoints=True))\n",
    "                s80_confidence_intervals = confidence_grouped.apply(lambda g: likelihood.confidence_interval_from_confidence_mask(g, 's80', include_endpoints=True))\n",
    "                SAR_confidence_intervals = confidence_grouped.apply(lambda g: likelihood.confidence_interval_from_confidence_mask(g, 'SAR', include_endpoints=True))\n",
    "\n",
    "                fits['p80_interval'] = (p80_confidence_intervals)\n",
    "                fits['s80_interval'] = (s80_confidence_intervals)\n",
    "                fits['SAR_interval'] = (SAR_confidence_intervals)\n",
    "                fits['written sample size'] = sample_size\n",
    "                fit_collections[hypothesis_name].append(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = (4,5,6)\n",
    "sample_size = 3600\n",
    "population_per_size = sample_size // len(population_mix)\n",
    "{s:population_per_size//s for s in mix}\n",
    "\n",
    "# mix (2,3,4): average size = 2.77\n",
    "# American including 2: average size = 3.0\n",
    "# mix (3,4,5): average size = 3.83\n",
    "# Guatemala including 2: average size = 4.4\n",
    "# mix (4,5,6): average size = 4.86\n",
    "# American\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guatemala_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine results into single dfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = {} \n",
    "for hypothesis_name, fit_dfs in fit_collections.items():\n",
    "    fit_df = pd.concat(fit_dfs)\n",
    "    fits[hypothesis_name] = fit_df\n",
    "    \n",
    "null_fit_df = fits['null hypothesis']\n",
    "fit_df = fits['all']\n",
    "#fit_df = fits['SAR and infectivity vary']\n",
    "fit_df[fit_df['written sample size'] == 5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical benchmarks from the fits\n",
    "\n",
    "We want to calculate various statistics to report as the result of our \"benchmarking\" in the paper. The first group of these concerns when the sample size is 5000. We pull out the relevant slice of the `fit_df` as needed to make these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first statistic is what % of the time the MLE for SAR is within 5% of the true value with high heterogeneity present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_slice = fit_df[(fit_df['written sample size'] == 5000) & (fit_df['parameters'] == (0.2, 0.2, 0.2))]\n",
    "sar_estimates = relevant_slice['MLE_SAR']\n",
    "((sar_estimates <= 0.25) & (sar_estimates >= 0.15)).sum() / len(relevant_slice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next statistic is the median width of the confidence interval for the MLE in SAR (also with high heterogeneity present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = []\n",
    "for confidence_range in relevant_slice['SAR_interval']:\n",
    "    lower, upper = confidence_range\n",
    "    widths.append(upper-lower)\n",
    "\n",
    "widths = np.array(widths)\n",
    "np.median(widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next statistic is \"for what value of $X$ is it true that 95% of estimates for $p_{80}$ fall within the range $\\pm X\\%$ of the true value when $p_{80}$ in fact equals $20\\%$ (but $s_{80}$ may or may not vary).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one heterogeneity\n",
    "relevant_slice = fit_df[(fit_df['written sample size'] == 5000) & (fit_df['parameters'] == (0.8, 0.2, 0.2))]\n",
    "x = 0.16\n",
    "((relevant_slice['MLE_p80'] >= 0.2 - x) & (relevant_slice['MLE_p80'] <= 0.2 + x)).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both heterogeneities\n",
    "relevant_slice = fit_df[(fit_df['written sample size'] == 5000) & (fit_df['parameters'] == (0.2, 0.2, 0.2))]\n",
    "x = 0.30\n",
    "((relevant_slice['MLE_p80'] >= 0.2 - x) & (relevant_slice['MLE_p80'] <= 0.2 + x)).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next statistic is the same as above but for $s_{80}$ instead of $p_{80}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one heterogeneity\n",
    "relevant_slice = fit_df[(fit_df['written sample size'] == 5000) & (fit_df['parameters'] == (0.2, 0.8, 0.2))]\n",
    "x = 0.18\n",
    "((relevant_slice['MLE_s80'] >= 0.2 - x) & (relevant_slice['MLE_s80'] <= 0.2 + x)).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both heterogeneities\n",
    "relevant_slice = fit_df[(fit_df['written sample size'] == 5000) & (fit_df['parameters'] == (0.2, 0.2, 0.2))]\n",
    "x = 0.28\n",
    "((relevant_slice['MLE_s80'] >= 0.2 - x) & (relevant_slice['MLE_s80'] <= 0.2 + x)).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we consider cases when the sample size is $1000$ and we want to know if the presence or absence of heterogeneity can be reliably determined. Our first statistic in this class is \"if $p_{80} = 0.8$ and $s_{80} = 0.2$, what % of the time do is the MLE for $p_{80} \\ge 60\\%$?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_slice = fit_df[(fit_df['written sample size'] == 1000) & (fit_df['parameters'] == (0.2, 0.8, 0.2))]\n",
    "(relevant_slice['MLE_p80'] >= 0.60).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same swoop (for the same experiment) we can ask, \"what % of the time is the MLE for $s_{80} \\le 0.40$?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_slice = fit_df[(fit_df['written sample size'] == 1000) & (fit_df['parameters'] == (0.2, 0.8, 0.2))]\n",
    "(relevant_slice['MLE_s80'] <= 0.40).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next statistic in this class is \"if $s_{80} = 0.8$ and $p_{80} = 0.2$, what % of the time do is the MLE for $s_{80} \\ge 60\\%$?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_slice = fit_df[(fit_df['written sample size'] == 1000) & (fit_df['parameters'] == (0.8, 0.2, 0.2))]\n",
    "(relevant_slice['MLE_s80'] >= 0.60).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, we ask the opposite end of the question two cells before (what % of the time can well there is significant $p_{80}$): \"what % of the time is the MLE for $p_{80} \\le 0.40$?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_slice = fit_df[(fit_df['written sample size'] == 1000) & (fit_df['parameters'] == (0.8, 0.2, 0.2))]\n",
    "(relevant_slice['MLE_p80'] <= 0.40).sum() / len(relevant_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will plot violins for each parameter over a single population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "plt.close('all')\n",
    "\n",
    "dpi = 100\n",
    "save_figs = False\n",
    "\n",
    "fig_sizes = {'small':(4,2.25), 'big':(8,4.5)}\n",
    "chosen_size = 'small'\n",
    "\n",
    "#%matplotlib agg\n",
    "min_dict = {0.6: 0.3, 0.4:0.10, 0.5: 0.15, 0.8: 0.5}\n",
    "max_dict = {0.6: 0.8, 0.4:0.65, 0.5: 0.8, 0.8: 0.8}\n",
    "colors = ['tab:green', 'tab:orange', 'tab:blue']\n",
    "axline_colors = ['palegreen', 'bisque', 'lightsteelblue']\n",
    "xlabels = ['MLE\\ns80', 'MLE\\np80', 'MLE\\nSAR', 'constant traits \\nMLE for SAR']\n",
    "grouping =['population mix', 'sample size', 'parameters']\n",
    "\n",
    "for k,g in fit_df.groupby(grouping):\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(1,4, dpi=dpi, sharey=True, figsize=fig_sizes[chosen_size])\n",
    "\n",
    "    population_mix, sample_size, parameters = k\n",
    "    population_name = None\n",
    "    if isinstance(population_mix, str):\n",
    "        population_name = population_mix\n",
    "        description = population_descriptions[population_mix]\n",
    "    else:\n",
    "        description = 'equally mixed'\n",
    "    if population_name:\n",
    "        plt.suptitle(f\"{sample_size} individuals {description}\\ns80, p80, SAR={parameters}\")\n",
    "    else:\n",
    "        plt.suptitle(f\"{sample_size} individuals {description} among size {population_mix} hhs. \\ns80, p80, SAR={parameters}\")\n",
    "    i = 0\n",
    "    for c in g.columns:\n",
    "        if 'MLE' not in c:\n",
    "            continue\n",
    "        if 's80' in c or 'p80' in c:\n",
    "            parameter_index = 0 if 's80' in c else 1\n",
    "            #mi, ma = min_dict[parameters[parameter_index]], max_dict[parameters[parameter_index]]\n",
    "            mi, ma = 0.1, 0.8\n",
    "            axes[i].set_ylim(mi, ma)\n",
    "        if 'SAR' in c:\n",
    "            pass\n",
    "            #axes[i].set_ylim(0.1, 0.5)\n",
    "            axes[i].set_ylim(0.1, 0.8)\n",
    "        if parameters[i] == 0.8:\n",
    "            axes[i].axhline(0.795, color=axline_colors[i])\n",
    "        else:\n",
    "            axes[i].axhline(parameters[i], color=axline_colors[i])\n",
    "        sns.violinplot(y=c, data=g, ax=axes[i], orient=\"v\", color=colors[i])\n",
    "        axes[i].set(xlabel=xlabels[i], ylabel='')\n",
    "        i += 1\n",
    "    null_fit_slice = null_fit_df.groupby(['population mix', 'sample size', 'parameters']).get_group(k)\n",
    "    #axes[3].set_ylim(0.1, 0.5)\n",
    "    axes[3].set_ylim(0.1, 0.8)\n",
    "    # plot the true SAR on the null hypothesis SAR\n",
    "    axes[3].axhline(parameters[2], color='mistyrose')\n",
    "    axes[3].set(xlabel=xlabels[3], ylabel='')\n",
    "    sns.violinplot(y=null_fit_slice['MLE_SAR'], data=null_fit_slice, ax=axes[3], orient=\"v\", color='indianred')\n",
    "    #axes[3].set(ylabel='MLE of SAR assuming no heterogeneity')\n",
    "    fig.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join('./figures', f'{k}' + '.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will plot violins for all the parameters over a variety of different populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "plt.close('all')\n",
    "\n",
    "#hypothesis = 'SAR and susceptibility vary'\n",
    "hypothesis = 'all'\n",
    "fit_df = fits[hypothesis]\n",
    "\n",
    "dpi = 300\n",
    "save_figs = True\n",
    "\n",
    "fig_sizes = {'small':(4,2.25), 'big':(10,4), 'vertical labels': (10,6)}\n",
    "chosen_size = 'big'\n",
    "\n",
    "#%matplotlib agg\n",
    "min_dict = {0.6: 0.3, 0.4:0.10, 0.5: 0.15, 0.8: 0.5}\n",
    "max_dict = {0.6: 0.8, 0.4:0.65, 0.5: 0.8, 0.8: 0.8}\n",
    "colors = ['tab:green', 'tab:orange', 'tab:blue', 'tab:blue']\n",
    "axline_colors = ['palegreen', 'bisque', 'lightsteelblue', 'lightsteelblue']\n",
    "xlabels = ['MLE\\ns80', 'MLE\\np80', 'MLE\\nSAR', 'constant traits \\nMLE for SAR']\n",
    "\n",
    "#population_mixes = [(2,3,4,), 'America_census_incl_2', (3,4,5,), 'Guatemala_incl_2', (4,5,6,)]\n",
    "\n",
    "drop_rules = {\n",
    "    'population mix': ['America_census_incl_2', 'Guatemala_incl_2'],\n",
    "}\n",
    "\n",
    "def apply_drop_rules(df, drop_rules):\n",
    "    new_df = df.copy()\n",
    "    for column,exclusions in drop_rules.items():\n",
    "        for exclusion in exclusions:\n",
    "            new_df = new_df[new_df[column] != exclusion]\n",
    "    return new_df\n",
    "\n",
    "# Use the 'drop_rules' dictionary to forget any populations we don't care about to render a cleaned up figure\n",
    "new_fit_df = apply_drop_rules(fit_df, drop_rules)\n",
    "new_fit_df['population mix'] = new_fit_df['population mix'].astype(str)\n",
    "null_hypothesis_fit_df = apply_drop_rules(fits['null hypothesis'], drop_rules)\n",
    "\n",
    "# add the 'null hypothesis' SAR estimates into the dataframe so they'll be an accessible column in plotting\n",
    "null_SARs = null_hypothesis_fit_df['MLE_SAR']\n",
    "null_SARs.name = 'MLE_SAR no heterogeneity'\n",
    "new_fit_df = pd.concat([new_fit_df, null_SARs], axis=1)\n",
    "\n",
    "grouping=['written sample size', 'parameters']\n",
    "for key,group in new_fit_df.groupby(grouping):\n",
    "    sample_size, parameters = key\n",
    "    # The true SAR is the same regardless of null hypothesis vs standard hypothesis\n",
    "    # but the `key` in the index isn't long enough because it doesn't know that we care about the null hypothesis\n",
    "    # so we add the true SAR as the line for \"actual value\" for the null hypothesis subplot\n",
    "    parameters = list(parameters) + [parameters[-1]]\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(1,len(results.metadata.parameters)+1, dpi=dpi, sharey=True, figsize=fig_sizes[chosen_size])\n",
    "    #for ax in axes:\n",
    "    #    ax.set_xticklabels(ax.get_xticks(), rotation = 90)\n",
    "    null_fit_group = null_hypothesis_fit_df.groupby(grouping).get_group(key)\n",
    "    relevant_parameters = list(results.metadata.parameters) + ['SAR no heterogeneity']\n",
    "    for param_index,parameter in enumerate(relevant_parameters):\n",
    "        seaborn_parameter_name = parameter\n",
    "        plt.suptitle(f\"{sample_size} individuals \\ns80, p80, SAR={parameters}\")\n",
    "        sns.violinplot(x='population mix', y=f'MLE_{seaborn_parameter_name}', data=group, ax=axes[param_index], orient=\"v\", color=colors[param_index])\n",
    "        if parameters[param_index] == 0.8:\n",
    "            axes[param_index].axhline(0.795, color=axline_colors[param_index])\n",
    "        else:\n",
    "            axes[param_index].axhline(parameters[param_index], color=axline_colors[param_index])\n",
    "        mi, ma = 0.02, 0.8\n",
    "        axes[param_index].set_ylim(mi, ma)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylabel(ax.get_ylabel().replace('_', ' '))\n",
    "        ax.set_xlabel('Household sizes')\n",
    "    \n",
    "    axes[param_index].axhline(0.18, color=axline_colors[param_index])\n",
    "\n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join('./figures', f'{key}' + '.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will plot violins for all the parameters over a group of sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "plt.close('all')\n",
    "\n",
    "#hypothesis = 'SAR and susceptibility vary'\n",
    "hypothesis = 'all'\n",
    "fit_df = fits[hypothesis]\n",
    "\n",
    "dpi = 400\n",
    "save_figs = True\n",
    "\n",
    "fig_sizes = {'small':(4,2.25), 'big':(10,4)}\n",
    "chosen_size = 'big'\n",
    "\n",
    "#%matplotlib agg\n",
    "min_dict = {0.6: 0.3, 0.4:0.10, 0.5: 0.15, 0.8: 0.5}\n",
    "max_dict = {0.6: 0.8, 0.4:0.65, 0.5: 0.8, 0.8: 0.8}\n",
    "colors = ['tab:green', 'tab:orange', 'tab:blue', 'tab:blue']\n",
    "axline_colors = ['palegreen', 'bisque', 'lightsteelblue', 'lightsteelblue']\n",
    "xlabels = ['MLE\\ns80', 'MLE\\np80', 'MLE\\nSAR', 'constant traits \\nMLE for SAR']\n",
    "\n",
    "chosen_pop = 'America_census_incl_2'\n",
    "\n",
    "def select_pop(df, chosen_pop):\n",
    "    new_df = df.copy()\n",
    "    new_df = new_df[new_df['population mix'] == chosen_pop]\n",
    "    return new_df\n",
    "\n",
    "# Choose only one population for plotting\n",
    "new_fit_df = select_pop(fit_df, chosen_pop)\n",
    "print(new_fit_df)\n",
    "new_fit_df['population mix'] = new_fit_df['population mix'].astype(str)\n",
    "\n",
    "# add the 'null hypothesis' SAR estimates into the dataframe so they'll be an accessible column in plotting\n",
    "# but first select the population in the null hypothesis frequencies as well\n",
    "null_hypothesis_fit_df = select_pop(fits['null hypothesis'], chosen_pop)\n",
    "null_SARs = null_hypothesis_fit_df['MLE_SAR']\n",
    "null_SARs.name = 'MLE_SAR no heterogeneity'\n",
    "print(null_SARs)\n",
    "new_fit_df = pd.concat([new_fit_df, null_SARs], axis=1)\n",
    "\n",
    "grouping=['parameters']\n",
    "if save_figs:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    new_fit_pq = pa.Table.from_pandas(new_fit_df)\n",
    "    pq.write_table(new_fit_pq, os.path.join('./figures/violin_df.parquet'))\n",
    "    pq.write_table(new_fit_pq, os.path.join('./figures/violin_df_null_hypothesis.parquet'))\n",
    "    \n",
    "for key,group in new_fit_df.groupby(grouping):\n",
    "    print(group)\n",
    "    parameters = key\n",
    "    # The true SAR is the same regardless of null hypothesis vs standard hypothesis\n",
    "    # but the `key` in the index isn't long enough because it doesn't know that we care about the null hypothesis\n",
    "    # so we add the true SAR as the line for \"actual value\" for the null hypothesis subplot\n",
    "    parameters = list(parameters) + [parameters[-1]]\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(1,len(results.metadata.parameters)+1, dpi=dpi, sharey=True, figsize=fig_sizes[chosen_size])\n",
    "    null_fit_group = null_hypothesis_fit_df.groupby(grouping).get_group(key)\n",
    "    relevant_parameters = list(results.metadata.parameters) + ['SAR no heterogeneity']\n",
    "    for param_index,parameter in enumerate(relevant_parameters):\n",
    "        seaborn_parameter_name = parameter\n",
    "        plt.suptitle(f\"\\ns80, p80, SAR={parameters}\")\n",
    "        #sns.violinplot(x='written sample size', y=f'MLE_{seaborn_parameter_name}', data=group, ax=axes[param_index], orient=\"v\", color=colors[param_index], cut=0.55)\n",
    "        sns.boxplot(x='written sample size', y=f'MLE_{seaborn_parameter_name}', data=group, ax=axes[param_index], orient=\"v\", color=colors[param_index])\n",
    "        if parameters[param_index] == 0.8:\n",
    "            axes[param_index].axhline(0.795, color=axline_colors[param_index])\n",
    "        else:\n",
    "            axes[param_index].axhline(parameters[param_index], color=axline_colors[param_index])\n",
    "        mi, ma = 0.0, 0.8\n",
    "        axes[param_index].set_ylim(mi, ma)\n",
    "    if save_figs:\n",
    "        plt.savefig(os.path.join('./figures', f'{key}' + '.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = fit_df[(fit_df['population mix'] == (4,5,6,7,8)) & (fit_df['parameters'] == (0.4, 0.8, 0.25))]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.catplot(x='sample size', y='MLE_p80', kind='swarm', data=relevant_data, s=3)\n",
    "plt.ylim((0.1, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_fits = null_fit_df['MLE_SAR'].copy()\n",
    "null_fits.name = 'No traits \\n MLE for SAR'\n",
    "version_2_df = pd.concat([fit_df, null_fits], axis=1)\n",
    "\n",
    "params = ['population mix', 'sample size', 'parameters']\n",
    "for k,g in version_2_df.groupby(params):\n",
    "    data = g.drop(params, axis=1)\n",
    "    plt.figure(dpi=800)\n",
    "    fig = sns.violinplot(data=data)\n",
    "    fig.set_ylim(0.1, 0.8)\n",
    "    plt.savefig(os.path.join('./figures', 'v2' + f'{k}' + '.jpg'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = fit_df.groupby(['population mix', 'sample size', 'parameters']).mean()\n",
    "stds = fit_df.groupby(['population mix', 'sample size', 'parameters']).std()\n",
    "stds.columns = ['STD_' + s for s in stds.columns]\n",
    "\n",
    "statistics = pd.concat([means, stds], axis=1)\n",
    "with open('./figures/stats.csv', 'w') as f:\n",
    "    statistics.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds.sort_values(by='STD_MLE_s80')\n",
    "with open('./figures/stds.csv', 'w') as f:\n",
    "    stds.to_csv(f)\n",
    "\n",
    "with open('./figures/means.csv', 'w') as f:\n",
    "    means.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power calculation\n",
    "\n",
    "The cells below are used to calculate the power of an intervention that changes the $\\text{SAR}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.likelihood as likelihood\n",
    "\n",
    "def SAR_pvalue_for_trial(baseline_logl, comparison_logl, for_increase=False):\n",
    "    baseline_posterior = np.exp(baseline_logl.sort_values(ascending=False)-baseline_logl.max())\n",
    "    baseline_posterior = baseline_posterior/baseline_posterior.sum()\n",
    "    # we groupby 'SAR' and sum so that we can capture all the probability at that SAR — regardless of other parameter values\n",
    "    baseline_probability_over_sars = baseline_posterior.groupby('SAR').sum()\n",
    "\n",
    "    comparison_posterior = np.exp(comparison_logl.sort_values(ascending=False)-comparison_logl.max())\n",
    "    #print(baseline_logl.idxmax(), comparison_logl.idxmax())\n",
    "    #if baseline_logl.idxmax()[3] == 0.01:\n",
    "    #    import pdb; pdb.set_trace()\n",
    "    #import pdb; pdb.set_trace()\n",
    "    baseline_SAR_confidence_interval = likelihood.confidence_interval_from_confidence_mask(likelihood.confidence_mask_from_logl(baseline_logl, percentiles=(0.9,)), key='SAR')\n",
    "    comparison_SAR_confidence_interval = likelihood.confidence_interval_from_confidence_mask(likelihood.confidence_mask_from_logl(comparison_logl, percentiles=(0.9,)), key='SAR')\n",
    "    comparison_posterior = comparison_posterior/comparison_posterior.sum()\n",
    "    probability_over_sars = comparison_posterior.groupby('SAR').sum()\n",
    "\n",
    "    # use the probability surface to generate imagined MLEs\n",
    "    sample1 = np.random.choice(baseline_probability_over_sars.index, 10000, p=baseline_probability_over_sars)\n",
    "    sample2 = np.random.choice(probability_over_sars.index, 10000, p=probability_over_sars)\n",
    "\n",
    "    # what fraction of the time does the first group have a increased/decreased SAR compared to the second group\n",
    "    if for_increase:\n",
    "        pvalue = np.count_nonzero((sample2-sample1) > 0)/len(sample1)\n",
    "    else:\n",
    "        pvalue = np.count_nonzero((sample2-sample1) < 0)/len(sample1)\n",
    "\n",
    "    return pvalue, baseline_SAR_confidence_interval, comparison_SAR_confidence_interval\n",
    "\n",
    "interval_notes = defaultdict(list)\n",
    "\n",
    "def calculate_power_over_SAR_range(population, trials, basline_parameters, sar_range, hypotheses, for_increase=False):\n",
    "    pvalue_sets = []\n",
    "    for hypothesis_name in hypotheses.keys():\n",
    "        frequencies = frequencies_by_hypothesis[hypothesis_name]\n",
    "        for sar in sar_range:\n",
    "            # replace baseline sar with target sar\n",
    "            parameters = list(basline_parameters)\n",
    "            parameters[results.metadata.parameters.index('SAR')] = float(f'{sar:0.3f}')\n",
    "            parameters = tuple(parameters)\n",
    "            #print(parameters)\n",
    "   \n",
    "            # get imagined infections from the simulated data at the baseline parameters to establish the probability surface for the MLE w.r.t. the baseline\n",
    "            samples = results.resample(basline_parameters, population, trials=trials)\n",
    "            baseline_logl = likelihood.logl_from_frequencies_and_counts(frequencies, samples['count'], results.metadata.parameters)\n",
    "\n",
    "            # get imagined infections from the simulated data at the comparison parameters to establish the probability surface for the MLE w.r.t. the comparison point\n",
    "            samples = results.resample(parameters, population, trials=trials)\n",
    "            logl = likelihood.logl_from_frequencies_and_counts(frequencies, samples['count'], results.metadata.parameters)\n",
    "\n",
    "            comparison_logl_grouped = logl.groupby('trial')\n",
    "            single_trial_pvalues = []\n",
    "            for key, baseline_logl_trial_group in baseline_logl.groupby('trial'):\n",
    "                comparison_logl_trial_group = comparison_logl_grouped.get_group(key)\n",
    "                pvalue, baseline_SAR_confidence_interval, comparison_SAR_confidence_interval = SAR_pvalue_for_trial(baseline_logl_trial_group, comparison_logl_trial_group, for_increase=for_increase)\n",
    "                single_trial_pvalues.append(pvalue)\n",
    "            #index = pd.MultiIndex.from_product([sar, hypothesis_name, list(range(trials))], names=['SAR', 'hypothesis', 'trial'])\n",
    "            #pvalue_sets.append(pd.Series(data=single_trial_pvalues, index=index))\n",
    "            pvalue_sets.append(pd.DataFrame({'pvalue':single_trial_pvalues, 'SAR':sar, 'hypothesis':hypothesis_name, 'trial':list(range(trials))}))\n",
    "    df_piece = pd.concat(pvalue_sets)\n",
    "    return df_piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_range = np.linspace(0.10, 0.15, 2)\n",
    "trials = 1000\n",
    "power_pvalue = 0.9\n",
    "\n",
    "# no, medium, and high heterogeneity as defined in the paper\n",
    "baseline_parameter_sets = [\n",
    "    (0.8, 0.8, 0.25),\n",
    "    (0.5, 0.5, 0.25),\n",
    "    (0.2, 0.2, 0.25),\n",
    "]\n",
    "\n",
    "populations = [\n",
    "    #{8:25},\n",
    "    #{4:50},\n",
    "    #{2:100},\n",
    "    #{8:125},\n",
    "    #{2:500},\n",
    "    #{2:33, 4:17, 8:8},\n",
    "    #{3:83, 4:63, 5:50, 6:42},\n",
    "    #{4:83, 6:56, 8:42}\n",
    "    #{4:250},\n",
    "    population_mix_to_population(named_populations['America_census_incl_2'], 200),\n",
    "    population_mix_to_population(named_populations['America_census_incl_2'], 1000)\n",
    "    #population_mix_to_population(named_populations['Guatemala_incl_2'], 200),\n",
    "    #population_mix_to_population(named_populations['Guatemala_incl_2'], 1000)\n",
    "]\n",
    "\n",
    "hypotheses = {\n",
    "    'all': ['s80', 'p80', 'SAR'],\n",
    "}\n",
    "frequencies_by_hypothesis = {k: restrict_parameters(results, included_parameters) for k,included_parameters in hypotheses.items()}\n",
    "\n",
    "pvalue_dfs = []\n",
    "from collections import defaultdict\n",
    "power_dfs = defaultdict(list)\n",
    "\n",
    "pvalue_df_pieces = []\n",
    "for baseline_parameters in baseline_parameter_sets:\n",
    "    for population in populations:\n",
    "        print(population)\n",
    "        pvalue_df_piece = calculate_power_over_SAR_range(population, trials, baseline_parameters, sar_range, hypotheses)\n",
    "        pvalue_df_piece['parameters'] = str(baseline_parameters)\n",
    "        pvalue_df_piece['population'] = str(population)\n",
    "        #print(pvalue_df_piece)\n",
    "        pvalue_df_pieces.append(pvalue_df_piece)\n",
    "        #pvalue_df = pd.DataFrame(pvalues_for_decrease, index=[float(f'{sar:0.3f}') for sar in sar_range]).transpose()\n",
    "        #pvalue_dfs.append(pvalue_df)\n",
    "        #power = ((pvalue_df > power_pvalue).sum()/trials)\n",
    "        #power.name = str(population)\n",
    "        #power_dfs[baseline_parameters].append(power)\n",
    "    #import pdb; pdb.set_trace()\n",
    "pvalue_df = pd.concat(pvalue_df_pieces)\n",
    "pvalue_df = pvalue_df.set_index(['population', 'parameters', 'hypothesis', 'SAR', 'trial']).squeeze().unstack([0,1,2,3])\n",
    "pvalue_df = (pvalue_df > 0.9).sum()/trials\n",
    "pvalue_df.name = 'power'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a little quick rearrangement to print a pretty table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_df.unstack([1,2]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save that table if desired to an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_df.unstack([1,2]).round(2).to_excel('./figures/powers/powers_1000_trials_SAR_25_America_fixed_0s_fixed_pop_part2.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e0418455f80b5f7c07ffd089975cbf09b69b8ff5f9769ad65d5982a17cefd6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
